{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "# Load list of slugs\n",
    "with open(\"top_slugs.txt\") as f:\n",
    "    slugs = [line.strip() for line in f.readlines()]\n",
    "\n",
    "# Initialize matrix\n",
    "n = len(slugs)\n",
    "slug_set = set(slugs)\n",
    "link_matrix = pd.DataFrame(0, index=slugs, columns=slugs)\n",
    "\n",
    "# Wikipedia base URL\n",
    "base_url = \"https://en.wikipedia.org/wiki/\"\n",
    "\n",
    "excluded_sections = [\"See also\", \"References\", \"Sources\", \"Further reading\", \"External links\"]\n",
    "\n",
    "def remove_after_first_excluded_section(content, excluded_sections):\n",
    "    \"\"\"Remove everything after the first encountered excluded section.\"\"\"\n",
    "    # Find all section headers\n",
    "    headers = content.find_all(['h2', 'h3'])\n",
    "    \n",
    "    for header in headers:\n",
    "        section_title = header.get_text().strip()\n",
    "        \n",
    "        if section_title in excluded_sections:\n",
    "            # Found the first excluded section - remove this header and everything after it\n",
    "            elements_to_remove = []\n",
    "            \n",
    "            # Get all elements that come after this header in the document\n",
    "            for element in header.find_all_next():\n",
    "                elements_to_remove.append(element)\n",
    "            \n",
    "            # Also remove the header itself\n",
    "            elements_to_remove.append(header)\n",
    "            \n",
    "            # Remove all collected elements\n",
    "            for element in elements_to_remove:\n",
    "                if element and element.parent:\n",
    "                    element.extract()\n",
    "            \n",
    "            break  # Stop after finding the first excluded section\n",
    "\n",
    "# Loop through each person's Wikipedia page\n",
    "for source_slug in slugs:\n",
    "    print(f\"Processing: {source_slug}\")\n",
    "    try:\n",
    "        url = base_url + source_slug\n",
    "        response = requests.get(url)\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "        # Find the main content div\n",
    "        content = soup.find('div', {'id': 'mw-content-text'})\n",
    "        if not content:\n",
    "            continue\n",
    "\n",
    "        # Remove everything after first excluded section\n",
    "        remove_after_first_excluded_section(content, excluded_sections)\n",
    "        \n",
    "        # Now find all remaining links\n",
    "        links = content.find_all('a', href=True)\n",
    "        linked_slugs = set(\n",
    "            link['href'].split('/wiki/')[-1].split('#')[0]\n",
    "            for link in links\n",
    "            if link['href'].startswith('/wiki/') and ':' not in link['href']\n",
    "        )\n",
    "\n",
    "        # Check if any link matches a target slug\n",
    "        for target_slug in slugs:\n",
    "            if target_slug != source_slug and target_slug in linked_slugs:\n",
    "                link_matrix.at[source_slug, target_slug] = 1\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Failed for {source_slug}: {e}\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "children = list(content.children)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(children[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(children[0].children)[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linked_slugs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "# Load list of slugs\n",
    "with open(\"top_slugs.txt\") as f:\n",
    "    slugs = [line.strip() for line in f.readlines()]\n",
    "\n",
    "# Initialize matrix\n",
    "n = len(slugs)\n",
    "slug_set = set(slugs)\n",
    "link_matrix = pd.DataFrame(0, index=slugs, columns=slugs)\n",
    "# Wikipedia base URL\n",
    "base_url = \"https://en.wikipedia.org/wiki/\"\n",
    "\n",
    "# Sections to exclude\n",
    "excluded_sections = [\"See also\", \"References\", \"Sources\", \"Further reading\", \"External links\"]\n",
    "\n",
    "\n",
    "def remove_excluded_sections(content, excluded_sections):\n",
    "    \"\"\"Remove excluded sections and their content from the HTML.\"\"\"\n",
    "    # Find all section headers\n",
    "    all_headers = content.find_all(['h2', 'h3'])\n",
    "    \n",
    "    # Create a list of (header, next_header) pairs\n",
    "    header_pairs = []\n",
    "    for i, header in enumerate(all_headers):\n",
    "        next_header = all_headers[i + 1] if i + 1 < len(all_headers) else None\n",
    "        header_pairs.append((header, next_header))\n",
    "    \n",
    "    # Process each header pair\n",
    "    for header, next_header in header_pairs:\n",
    "        section_title = header.get_text().strip()\n",
    "        \n",
    "        if section_title in excluded_sections:\n",
    "            # Find all elements between this header and the next header\n",
    "            elements_to_remove = [header]  # Start with the header itself\n",
    "            \n",
    "            # Get all elements in the document after this header\n",
    "            all_elements = header.find_all_next()\n",
    "            \n",
    "            for element in all_elements:\n",
    "                # Stop if we reach the next header\n",
    "                if next_header and element == next_header:\n",
    "                    break\n",
    "                # Stop if we reach any other section header that comes after next_header\n",
    "                if element.name in ['h2', 'h3'] and element != header:\n",
    "                    break\n",
    "                elements_to_remove.append(element)\n",
    "            \n",
    "            # Remove all collected elements\n",
    "            for element in elements_to_remove:\n",
    "                if element and element.parent:  # Make sure element still exists in DOM\n",
    "                    element.extract()\n",
    "\n",
    "# Loop through each person's Wikipedia page\n",
    "for source_slug in slugs:\n",
    "    print(f\"Processing: {source_slug}\")\n",
    "    try:\n",
    "        url = base_url + source_slug\n",
    "        response = requests.get(url)\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "        # Find the main content div\n",
    "        content = soup.find('div', {'id': 'mw-content-text'})\n",
    "        if not content:\n",
    "            continue\n",
    "\n",
    "        # Remove excluded sections\n",
    "        remove_excluded_sections(content, excluded_sections)\n",
    "        \n",
    "        # Now find all remaining links\n",
    "        links = content.find_all('a', href=True)\n",
    "        linked_slugs = set(\n",
    "            link['href'].split('/wiki/')[-1].split('#')[0]\n",
    "            for link in links\n",
    "            if link['href'].startswith('/wiki/') and ':' not in link['href']\n",
    "        )\n",
    "\n",
    "        # Check if any link matches a target slug\n",
    "        for target_slug in slugs:\n",
    "            if target_slug != source_slug and target_slug in linked_slugs:\n",
    "                link_matrix.at[source_slug, target_slug] = 1\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Failed for {source_slug}: {e}\")\n",
    "    break"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
